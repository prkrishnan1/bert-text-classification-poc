{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "773cbac1",
   "metadata": {},
   "source": [
    "# Introduction to JumpStart - Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eb1ffa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook. \n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-2/introduction_to_amazon_algorithms|jumpstart_text_classification|Amazon_JumpStart_Text_Classification.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8029d8a",
   "metadata": {},
   "source": [
    "---\n",
    "Welcome to Amazon [SageMaker JumpStart](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html)! You can use JumpStart to solve many Machine Learning tasks through one-click in SageMaker Studio, or through [SageMaker JumpStart API](https://sagemaker.readthedocs.io/en/stable/overview.html#use-prebuilt-models-with-sagemaker-jumpstart). \n",
    "\n",
    "In this demo notebook, we demonstrate how to use the JumpStart API for Text Classification. Text Classification refers to classifying an input sentence to one of the class labels of the training dataset.  We demonstrate two use cases of Text Classification models:\n",
    "\n",
    "* How to use a Transformer model pre-trained on English dataset, and fine-tuned on [SST2](https://nlp.stanford.edu/sentiment/index.html) dataset, to perform Sentiment Analysis.\n",
    "* How to fine-tune a pre-trained Transformer model to a custom dataset, and then run inference on the fine-tuned model.\n",
    "\n",
    "Note: This notebook was tested on ml.t3.medium instance in Amazon SageMaker Studio with Python 3 (Data Science) kernel and in Amazon SageMaker Notebook instance with conda_python3 kernel.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12d4028",
   "metadata": {},
   "source": [
    "1. [Set Up](#1.-Set-Up)\n",
    "2. [Select a pre-trained model](#2.-Select-a-pre-trained-model)\n",
    "3. [Run inference on the pre-trained model](#3.-Run-inference-on-the-pre-trained-model)\n",
    "    * [Retrieve JumpStart Artifacts & Deploy an Endpoint](#3.1.-Retrieve-JumpStart-Artifacts-&-Deploy-an-Endpoint)\n",
    "    * [Example input sentences for inference](#3.2.-Example-input-sentences-for-inference)\n",
    "    * [Query endpoint and parse response](#3.3.-Query-endpoint-and-parse-response)\n",
    "    * [Clean up the endpoint](#3.4.-Clean-up-the-endpoint)\n",
    "4. [Finetune the pre-trained model on a custom dataset](#4.-Finetune-the-pre-trained-model-on-a-custom-dataset)\n",
    "    * [Retrieve JumpStart Training artifacts](#4.1.-Retrieve-JumpStart-Training-artifacts)\n",
    "    * [Set Training parameters](#4.2.-Set-Training-parameters)\n",
    "    * [Train with Automatic Model Tuning (HPO)](#AMT)\n",
    "    * [Start Training](#4.4.-Start-Training)\n",
    "    * [Deploy & run Inference on the fine-tuned model](#4.5.-Deploy-&-run-Inference-on-the-fine-tuned-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664b058b",
   "metadata": {},
   "source": [
    "## 1. Set Up\n",
    "***\n",
    "Before executing the notebook, there are some initial steps required for setup. This notebook requires latest version of sagemaker and ipywidgets.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a24ba2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "autovizwidget 0.20.5 requires pandas<2.0.0,>=0.20.1, but you have pandas 2.0.3 which is incompatible.\r\n",
      "hdijupyterutils 0.20.5 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.0.3 which is incompatible.\r\n",
      "sparkmagic 0.20.5 requires nest-asyncio==1.5.5, but you have nest-asyncio 1.5.6 which is incompatible.\r\n",
      "sparkmagic 0.20.5 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.0.3 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker ipywidgets --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875eddf7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "To train and host on Amazon Sagemaker, we need to setup and authenticate the use of AWS services. Here, we use the execution role associated with the current notebook instance as the AWS account role with SageMaker access. It has necessary permissions, including access to your data in S3. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef71ff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "aws_role = get_execution_role()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8c4bbb",
   "metadata": {},
   "source": [
    "## 2. Select a pre-trained model\n",
    "***\n",
    "You can continue with the default model, or can choose a different model from the dropdown generated upon running the next cell. A complete list of JumpStart models can also be accessed at [JumpStart Models](https://sagemaker.readthedocs.io/en/stable/doc_utils/jumpstart.html#).\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc4016db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"tensorflow-tc-bert-en-uncased-L-12-H-768-A-12-2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102a1694",
   "metadata": {},
   "source": [
    "***\n",
    "[Optional] Select a different JumpStart model. Here, we download jumpstart model_manifest file from the jumpstart s3 bucket, filter-out all the Text Classification models and select a model.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "422bcdec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Select a JumpStart pre-trained model from the dropdown below"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1bf5ec7dae457997ce92f1d82c9414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='JumpStart Text Classification Models:', index=24, layout=Layout(width='max-content'), opâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import IPython\n",
    "from ipywidgets import Dropdown\n",
    "\n",
    "# download JumpStart model_manifest file.\n",
    "boto3.client(\"s3\").download_file(\n",
    "    f\"jumpstart-cache-prod-{aws_region}\", \"models_manifest.json\", \"models_manifest.json\"\n",
    ")\n",
    "with open(\"models_manifest.json\", \"rb\") as json_file:\n",
    "    model_list = json.load(json_file)\n",
    "\n",
    "# filter-out all the Text Classification models from the manifest list.\n",
    "tc_models_all_versions, tc_models = [\n",
    "    model[\"model_id\"] for model in model_list if \"-tc-\" in model[\"model_id\"]\n",
    "], []\n",
    "[tc_models.append(model) for model in tc_models_all_versions if model not in tc_models]\n",
    "\n",
    "# display the model-ids in a dropdown, for user to select a model.\n",
    "dropdown = Dropdown(\n",
    "    value=model_id,\n",
    "    options=tc_models,\n",
    "    description=\"JumpStart Text Classification Models:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout={\"width\": \"max-content\"},\n",
    ")\n",
    "display(IPython.display.Markdown(\"## Select a JumpStart pre-trained model from the dropdown below\"))\n",
    "display(dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03c11d0",
   "metadata": {},
   "source": [
    "## 3. Run inference on the pre-trained model\n",
    "***\n",
    "Using JumpStart, we can perform inference on the pre-trained model, even without fine-tuning it first on a custom dataset. For this example, that means on an input sentence, predicting the class label from one of the 2 classes of the [SST2](https://nlp.stanford.edu/sentiment/index.html) dataset. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f688890b",
   "metadata": {},
   "source": [
    "### 3.1. Retrieve JumpStart Artifacts & Deploy an Endpoint\n",
    "***\n",
    "We retrieve the deploy_image_uri, deploy_source_uri, and base_model_uri for the pre-trained model. To host the pre-trained model, we create an instance of [`sagemaker.model.Model`](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html) and deploy it.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15124093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----!"
     ]
    }
   ],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "# model_version=\"*\" fetches the latest version of the model.\n",
    "infer_model_id, infer_model_version = dropdown.value, \"*\"\n",
    "\n",
    "endpoint_name = name_from_base(f\"jumpstart-example-{infer_model_id}\")\n",
    "\n",
    "inference_instance_type = \"ml.m5.xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri.\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    image_scope=\"inference\",\n",
    "    model_id=infer_model_id,\n",
    "    model_version=infer_model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "# Retrieve the inference script uri.\n",
    "deploy_source_uri = script_uris.retrieve(\n",
    "    model_id=infer_model_id, model_version=infer_model_version, script_scope=\"inference\"\n",
    ")\n",
    "# Retrieve the base model uri.\n",
    "base_model_uri = model_uris.retrieve(\n",
    "    model_id=infer_model_id, model_version=infer_model_version, model_scope=\"inference\"\n",
    ")\n",
    "# Create the SageMaker model instance. Note that we need to pass Predictor class when we deploy model through Model class,\n",
    "# for being able to run inference through the sagemaker API.\n",
    "model = Model(\n",
    "    image_uri=deploy_image_uri,\n",
    "    source_dir=deploy_source_uri,\n",
    "    model_data=base_model_uri,\n",
    "    entry_point=\"inference.py\",\n",
    "    role=aws_role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=endpoint_name,\n",
    ")\n",
    "# deploy the Model.\n",
    "base_model_predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f602aa",
   "metadata": {},
   "source": [
    "### 3.2. Example input sentences for inference\n",
    "***\n",
    "These examples are taken from SST2 dataset downloaded from [TensorFlow](https://www.tensorflow.org/datasets/catalog/glue#gluesst2). [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). [Dataset Homepage](https://nlp.stanford.edu/sentiment/index.html). \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f31658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"astonishing ... ( frames ) profound ethical and philosophical questions in the form of dazzling pop entertainment\"\n",
    "text2 = \"simply stupid , irrelevant and deeply , truly , bottomlessly cynical \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a345f3d",
   "metadata": {},
   "source": [
    "### 3.3. Query endpoint and parse response\n",
    "***\n",
    "Input to the endpoint is a single sentence. Response from the endpoint is a dictionary containing the predicted class label, and a list of class label probabilities.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4c7d380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference:\n",
      "Input text: 'astonishing ... ( frames ) profound ethical and philosophical questions in the form of dazzling pop entertainment'\n",
      "Model prediction: [0.000452965876, 0.999547064]\n",
      "Labels: [0, 1]\n",
      "Predicted Label: \u001b[1m1\u001b[0m\n",
      "\n",
      "Inference:\n",
      "Input text: 'simply stupid , irrelevant and deeply , truly , bottomlessly cynical '\n",
      "Model prediction: [0.998723, 0.0012769578]\n",
      "Labels: [0, 1]\n",
      "Predicted Label: \u001b[1m0\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
    "\n",
    "\n",
    "def query_endpoint(encoded_text):\n",
    "    response = base_model_predictor.predict(\n",
    "        encoded_text, {\"ContentType\": \"application/x-text\", \"Accept\": \"application/json;verbose\"}\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_response(query_response):\n",
    "    model_predictions = json.loads(query_response)\n",
    "    probabilities, labels, predicted_label = (\n",
    "        model_predictions[\"probabilities\"],\n",
    "        model_predictions[\"labels\"],\n",
    "        model_predictions[\"predicted_label\"],\n",
    "    )\n",
    "    return probabilities, labels, predicted_label\n",
    "\n",
    "\n",
    "for text in [text1, text2]:\n",
    "    query_response = query_endpoint(text.encode(\"utf-8\"))\n",
    "    probabilities, labels, predicted_label = parse_response(query_response)\n",
    "    print(\n",
    "        f\"Inference:{newline}\"\n",
    "        f\"Input text: '{text}'{newline}\"\n",
    "        f\"Model prediction: {probabilities}{newline}\"\n",
    "        f\"Labels: {labels}{newline}\"\n",
    "        f\"Predicted Label: {bold}{predicted_label}{unbold}{newline}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ca5b06",
   "metadata": {},
   "source": [
    "### 3.4. Clean up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a455a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint and the attached resources\n",
    "base_model_predictor.delete_model()\n",
    "base_model_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b81eff",
   "metadata": {},
   "source": [
    "## 4. Finetune the pre-trained model on a custom dataset\n",
    "***\n",
    "Previously, we saw how to run inference on a pre-trained model, which was fine-tuned on SST dataset. Next, we discuss how a model can be finetuned to a custom dataset with any number of classes. \n",
    "\n",
    "The Text Embedding model can be fine-tuned on any text classification dataset in the same way the \n",
    "model available for inference has been fine-tuned on the SST2 movie review dataset.\n",
    "\n",
    "The model available for fine-tuning attaches a classification layer to the Text Embedding model \n",
    "and initializes the layer parameters to random values. \n",
    "The output dimension of the classification layer is determined based on the number of classes \n",
    "detected in the input data. The fine-tuning step fine-tunes all the model \n",
    "parameters to minimize prediction error on the input data and returns the fine-tuned model. \n",
    "The model returned by fine-tuning can be further deployed for inference. \n",
    "Below are the instructions for how the training data should be formatted for input to the model.\n",
    "\n",
    "\n",
    "- **Input:** A directory containing a 'data.csv' file. \n",
    "    - Each row of the first column of 'data.csv' should have integer class labels between 0 to the number of classes.\n",
    "    - Each row of the second column should have the corresponding text. \n",
    "- **Output:** A trained model that can be deployed for inference. \n",
    " \n",
    "Below is an example of 'data.csv' file showing values in its first two columns. Note that the file should not have any header.\n",
    "\n",
    "|   |   |\n",
    "|---|---|\n",
    "|0\t|hide new secretions from the parental units| \n",
    "|0\t|contains no wit , only labored gags| \n",
    "|1\t|that loves its characters and communicates something rather beautiful about human nature| \n",
    "|...|...|\n",
    " \n",
    "source: [TensorFlow Hub](model_url). License:[Apache 2.0 License](https://jumpstart-cache-alpha-us-west-2.s3-us-west-2.amazonaws.com/licenses/Apache-License/LICENSE-2.0.txt).\n",
    " \n",
    "SST2 dataset is downloaded from [TensorFlow](https://www.tensorflow.org/datasets/catalog/glue#gluesst2).\n",
    " [Apache 2.0 License](https://jumpstart-cache-prod-us-west-2.s3-us-west-2.amazonaws.com/licenses/Apache-License/LICENSE-2.0.txt).\n",
    "  [Dataset Homepage](https://nlp.stanford.edu/sentiment/index.html). \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a283fc",
   "metadata": {},
   "source": [
    "### 4.1. Retrieve JumpStart Training artifacts\n",
    "***\n",
    "Here, for the selected model, we retrieve the training docker container, the training algorithm source, the pre-trained model, and a python dictionary of the training hyper-parameters that the algorithm accepts with their default values. Note that the model_version=\"*\" fetches the lates model. Also, we do need to specify the training_instance_type to fetch train_image_uri.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e03ca957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "\n",
    "model_id, model_version = dropdown.value, \"*\"\n",
    "training_instance_type = \"ml.p3.2xlarge\"\n",
    "\n",
    "# Retrieve the docker image\n",
    "train_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    image_scope=\"training\",\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "# Retrieve the training script\n",
    "train_source_uri = script_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, script_scope=\"training\"\n",
    ")\n",
    "# Retrieve the pre-trained model tarball to further fine-tune\n",
    "train_model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"training\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "894042b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/tensorflow/transfer_learning/tc/v2.0.1/sourcedir.tar.gz'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_source_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d059ba",
   "metadata": {},
   "source": [
    "### 4.2. Set Training parameters\n",
    "***\n",
    "Now that we are done with all the setup that is needed, we are ready to fine-tune our Text Classification model. To begin, let us create a [``sageMaker.estimator.Estimator``](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) object. This estimator will launch the training job. \n",
    "\n",
    "There are two kinds of parameters that need to be set for training. \n",
    "\n",
    "The first one are the parameters for the training job. These include: (i) Training data path. This is S3 folder in which the input data is stored, (ii) Output path: This the s3 folder in which the training output is stored. (iii) Training instance type: This indicates the type of machine on which to run the training. Typically, we use GPU instances for these training. We defined the training instance type above to fetch the correct train_image_uri. \n",
    "\n",
    "The second set of parameters are algorithm specific training hyper-parameters.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e71128",
   "metadata": {},
   "source": [
    "#### Load, preprocess and store training data in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4890a46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "268259e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "782f01f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>short_description</th>\n",
       "      <th>cmdb_ci</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Excel Macro Progrem not Working</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td># [REM-GSB] I've received this message 3-4 tim...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>*[SDSS ONS] Wireless Network troubles in Mitch...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>iPad - how to mark as a shared device?</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[SWEEP] - SRWC A323</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25384</th>\n",
       "      <td>Image Macbook 13\" c02zj0x7lvdp</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25385</th>\n",
       "      <td>trying to ensure my computer is compliant</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25386</th>\n",
       "      <td>3 Cisco phones need to be picked up</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25387</th>\n",
       "      <td>[SWEEP] - SRWC U565</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25388</th>\n",
       "      <td>[SWEEP] - SRWC C531</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25389 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       short_description  cmdb_ci\n",
       "0                        Excel Macro Progrem not Working       13\n",
       "1      # [REM-GSB] I've received this message 3-4 tim...        3\n",
       "2      *[SDSS ONS] Wireless Network troubles in Mitch...        4\n",
       "3                 iPad - how to mark as a shared device?       13\n",
       "4                                   [SWEEP] - SRWC A323         2\n",
       "...                                                  ...      ...\n",
       "25384                     Image Macbook 13\" c02zj0x7lvdp       13\n",
       "25385          trying to ensure my computer is compliant        4\n",
       "25386                3 Cisco phones need to be picked up        9\n",
       "25387                               [SWEEP] - SRWC U565         2\n",
       "25388                               [SWEEP] - SRWC C531         2\n",
       "\n",
       "[25389 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "df_train = df_train[pd.notnull(df_train.cmdb_ci)]\n",
    "df_train = df_train[pd.notnull(df_train.short_description)]\n",
    "\n",
    "df_train.short_description = df_train.short_description.apply(lambda text: re.sub(' +', ' ', text))\n",
    "\n",
    "df_train.cmdb_ci = LabelEncoder().fit_transform(df_train.cmdb_ci)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c008bd6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cmdb_ci</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>Excel Macro Progrem not Working</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td># [REM-GSB] I've received this message 3-4 tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>*[SDSS ONS] Wireless Network troubles in Mitch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>iPad - how to mark as a shared device?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>[SWEEP] - SRWC A323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25384</th>\n",
       "      <td>13</td>\n",
       "      <td>Image Macbook 13\" c02zj0x7lvdp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25385</th>\n",
       "      <td>4</td>\n",
       "      <td>trying to ensure my computer is compliant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25386</th>\n",
       "      <td>9</td>\n",
       "      <td>3 Cisco phones need to be picked up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25387</th>\n",
       "      <td>2</td>\n",
       "      <td>[SWEEP] - SRWC U565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25388</th>\n",
       "      <td>2</td>\n",
       "      <td>[SWEEP] - SRWC C531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25389 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cmdb_ci                                  short_description\n",
       "0           13                    Excel Macro Progrem not Working\n",
       "1            3  # [REM-GSB] I've received this message 3-4 tim...\n",
       "2            4  *[SDSS ONS] Wireless Network troubles in Mitch...\n",
       "3           13             iPad - how to mark as a shared device?\n",
       "4            2                               [SWEEP] - SRWC A323 \n",
       "...        ...                                                ...\n",
       "25384       13                     Image Macbook 13\" c02zj0x7lvdp\n",
       "25385        4          trying to ensure my computer is compliant\n",
       "25386        9                3 Cisco phones need to be picked up\n",
       "25387        2                               [SWEEP] - SRWC U565 \n",
       "25388        2                               [SWEEP] - SRWC C531 \n",
       "\n",
       "[25389 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train[['cmdb_ci', 'short_description']]\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32039d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('data.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b717a4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "training_data_filepath = 'data.csv'\n",
    "training_data_bucket = sess.default_bucket()\n",
    "training_data_prefix = f'jumpstart_tc/training/{training_data_filepath}'\n",
    "training_dataset_s3_path = f\"s3://{training_data_bucket}/{training_data_prefix}\"\n",
    "\n",
    "s3.upload_file(training_data_filepath, training_data_bucket, training_data_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "59d8c279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample training data is available in this bucket\n",
    "\n",
    "\n",
    "output_bucket = sess.default_bucket()\n",
    "output_prefix = \"jumpstart-tc-training\"\n",
    "\n",
    "s3_output_location = f\"s3://{output_bucket}/{output_prefix}/output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eee784",
   "metadata": {},
   "source": [
    "***\n",
    "For algorithm specific hyper-parameters, we start by fetching python dictionary of the training hyper-parameters that the algorithm accepts with their default values. This can then be overridden to custom values.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b994020e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_only_top_layer': 'False',\n",
       " 'epochs': '4',\n",
       " 'batch_size': '64',\n",
       " 'optimizer': 'adamw',\n",
       " 'learning_rate': '2e-05',\n",
       " 'warmup_steps_fraction': '0.1',\n",
       " 'beta_1': '0.9',\n",
       " 'beta_2': '0.999',\n",
       " 'momentum': '0.9',\n",
       " 'epsilon': '1e-06',\n",
       " 'rho': '0.95',\n",
       " 'initial_accumulator_value': '0.1',\n",
       " 'early_stopping': 'False',\n",
       " 'early_stopping_patience': '5',\n",
       " 'early_stopping_min_delta': '0.0',\n",
       " 'dropout_rate': '0.2',\n",
       " 'regularizers_l2': '0.01',\n",
       " 'validation_split_ratio': '0.2',\n",
       " 'reinitialize_top_layer': 'Auto'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker import hyperparameters\n",
    "\n",
    "# Retrieve the default hyper-parameters for fine-tuning the model\n",
    "hyperparameters = hyperparameters.retrieve_default(model_id=model_id, model_version=model_version)\n",
    "\n",
    "# [Optional] Override default hyperparameters with custom values\n",
    "hyperparameters[\"batch_size\"] = \"64\"\n",
    "hyperparameters[\"epochs\"] = \"4\"\n",
    "\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8047531e",
   "metadata": {},
   "source": [
    "### 4.3. Train with Automatic Model Tuning ([HPO](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html)) <a id='AMT'></a>\n",
    "***\n",
    "Amazon SageMaker automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many training jobs on your dataset using the algorithm and ranges of hyperparameters that you specify. It then chooses the hyperparameter values that result in a model that performs the best, as measured by a metric that you choose. We will use a [HyperparameterTuner](https://sagemaker.readthedocs.io/en/stable/api/training/tuner.html) object to interact with Amazon SageMaker hyperparameter tuning APIs.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9bc8ea44",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.tuner import ContinuousParameter\n",
    "\n",
    "# Use AMT for tuning and selecting the best model\n",
    "use_amt = False\n",
    "\n",
    "# Define objective metric per framework, based on which the best model will be selected.\n",
    "metric_definitions_per_model = {\n",
    "    \"tensorflow\": {\n",
    "        \"metrics\": [{\"Name\": \"val_accuracy\", \"Regex\": \"val_accuracy: ([0-9\\\\.]+)\"}],\n",
    "        \"type\": \"Maximize\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# You can select from the hyperparameters supported by the model, and configure ranges of values to be searched for training the optimal model.(https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-ranges.html)\n",
    "hyperparameter_ranges = {\n",
    "    \"adam-learning-rate\": ContinuousParameter(0.00001, 0.01, scaling_type=\"Logarithmic\")\n",
    "}\n",
    "\n",
    "# Increase the total number of training jobs run by AMT, for increased accuracy (and training time).\n",
    "max_jobs = 6\n",
    "# Change parallel training jobs run by AMT to reduce total training time, constrained by your account limits.\n",
    "# if max_jobs=max_parallel_jobs then Bayesian search turns to Random.\n",
    "max_parallel_jobs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164ecdb6",
   "metadata": {},
   "source": [
    "### 4.4. Start Training\n",
    "***\n",
    "We start by creating the estimator object with all the required assets and then launch the training job.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b191e499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.interactive_apps.base_interactive_app:NOTEBOOK_METADATA_FILE detected but failed to get valid domain and user from it.\n",
      "INFO:sagemaker:Creating training-job with name: jumpstart-example-tensorflow-tc-bert-en-2023-08-31-17-26-54-055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-31 17:26:54 Starting - Starting the training job......\n",
      "2023-08-31 17:27:27 Starting - Preparing the instances for training......\n",
      "2023-08-31 17:28:37 Downloading - Downloading input data...\n",
      "2023-08-31 17:29:17 Training - Downloading the training image........................\n",
      "2023-08-31 17:33:03 Training - Training image download completed. Training in progress..\u001b[34m2023-08-31 17:33:25.624280: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2023-08-31 17:33:25.624493: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2023-08-31 17:33:25.668424: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2023-08-31 17:33:28,235 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2023-08-31 17:33:28,643 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": \"64\",\n",
      "        \"beta_1\": \"0.9\",\n",
      "        \"beta_2\": \"0.999\",\n",
      "        \"dropout_rate\": \"0.2\",\n",
      "        \"early_stopping\": \"False\",\n",
      "        \"early_stopping_min_delta\": \"0.0\",\n",
      "        \"early_stopping_patience\": \"5\",\n",
      "        \"epochs\": \"4\",\n",
      "        \"epsilon\": \"1e-06\",\n",
      "        \"initial_accumulator_value\": \"0.1\",\n",
      "        \"learning_rate\": \"2e-05\",\n",
      "        \"momentum\": \"0.9\",\n",
      "        \"optimizer\": \"adamw\",\n",
      "        \"regularizers_l2\": \"0.01\",\n",
      "        \"reinitialize_top_layer\": \"Auto\",\n",
      "        \"rho\": \"0.95\",\n",
      "        \"train_only_top_layer\": \"False\",\n",
      "        \"validation_split_ratio\": \"0.2\",\n",
      "        \"warmup_steps_fraction\": \"0.1\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"model\": {\n",
      "            \"ContentType\": \"application/x-sagemaker-model\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"jumpstart-example-tensorflow-tc-bert-en-2023-08-31-17-26-54-055\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/tensorflow/transfer_learning/tc/v2.0.1/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":\"64\",\"beta_1\":\"0.9\",\"beta_2\":\"0.999\",\"dropout_rate\":\"0.2\",\"early_stopping\":\"False\",\"early_stopping_min_delta\":\"0.0\",\"early_stopping_patience\":\"5\",\"epochs\":\"4\",\"epsilon\":\"1e-06\",\"initial_accumulator_value\":\"0.1\",\"learning_rate\":\"2e-05\",\"momentum\":\"0.9\",\"optimizer\":\"adamw\",\"regularizers_l2\":\"0.01\",\"reinitialize_top_layer\":\"Auto\",\"rho\":\"0.95\",\"train_only_top_layer\":\"False\",\"validation_split_ratio\":\"0.2\",\"warmup_steps_fraction\":\"0.1\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"model\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/tensorflow/transfer_learning/tc/v2.0.1/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"model\":\"/opt/ml/input/data/model\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":\"64\",\"beta_1\":\"0.9\",\"beta_2\":\"0.999\",\"dropout_rate\":\"0.2\",\"early_stopping\":\"False\",\"early_stopping_min_delta\":\"0.0\",\"early_stopping_patience\":\"5\",\"epochs\":\"4\",\"epsilon\":\"1e-06\",\"initial_accumulator_value\":\"0.1\",\"learning_rate\":\"2e-05\",\"momentum\":\"0.9\",\"optimizer\":\"adamw\",\"regularizers_l2\":\"0.01\",\"reinitialize_top_layer\":\"Auto\",\"rho\":\"0.95\",\"train_only_top_layer\":\"False\",\"validation_split_ratio\":\"0.2\",\"warmup_steps_fraction\":\"0.1\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"jumpstart-example-tensorflow-tc-bert-en-2023-08-31-17-26-54-055\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/tensorflow/transfer_learning/tc/v2.0.1/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"64\",\"--beta_1\",\"0.9\",\"--beta_2\",\"0.999\",\"--dropout_rate\",\"0.2\",\"--early_stopping\",\"False\",\"--early_stopping_min_delta\",\"0.0\",\"--early_stopping_patience\",\"5\",\"--epochs\",\"4\",\"--epsilon\",\"1e-06\",\"--initial_accumulator_value\",\"0.1\",\"--learning_rate\",\"2e-05\",\"--momentum\",\"0.9\",\"--optimizer\",\"adamw\",\"--regularizers_l2\",\"0.01\",\"--reinitialize_top_layer\",\"Auto\",\"--rho\",\"0.95\",\"--train_only_top_layer\",\"False\",\"--validation_split_ratio\",\"0.2\",\"--warmup_steps_fraction\",\"0.1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_BETA_1=0.9\u001b[0m\n",
      "\u001b[34mSM_HP_BETA_2=0.999\u001b[0m\n",
      "\u001b[34mSM_HP_DROPOUT_RATE=0.2\u001b[0m\n",
      "\u001b[34mSM_HP_EARLY_STOPPING=False\u001b[0m\n",
      "\u001b[34mSM_HP_EARLY_STOPPING_MIN_DELTA=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_EARLY_STOPPING_PATIENCE=5\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=4\u001b[0m\n",
      "\u001b[34mSM_HP_EPSILON=1e-06\u001b[0m\n",
      "\u001b[34mSM_HP_INITIAL_ACCUMULATOR_VALUE=0.1\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=2e-05\u001b[0m\n",
      "\u001b[34mSM_HP_MOMENTUM=0.9\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIMIZER=adamw\u001b[0m\n",
      "\u001b[34mSM_HP_REGULARIZERS_L2=0.01\u001b[0m\n",
      "\u001b[34mSM_HP_REINITIALIZE_TOP_LAYER=Auto\u001b[0m\n",
      "\u001b[34mSM_HP_RHO=0.95\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_ONLY_TOP_LAYER=False\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_SPLIT_RATIO=0.2\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_STEPS_FRACTION=0.1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python39.zip:/usr/local/lib/python3.9:/usr/local/lib/python3.9/lib-dynload:/usr/local/lib/python3.9/site-packages:/usr/local/lib/python3.9/site-packages/smdebug-1.0.26b20230119-py3.9.egg:/usr/local/lib/python3.9/site-packages/pyinstrument-3.4.2-py3.9.egg:/usr/local/lib/python3.9/site-packages/pyinstrument_cext-0.2.4-py3.9-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.9 transfer_learning.py --batch_size 64 --beta_1 0.9 --beta_2 0.999 --dropout_rate 0.2 --early_stopping False --early_stopping_min_delta 0.0 --early_stopping_patience 5 --epochs 4 --epsilon 1e-06 --initial_accumulator_value 0.1 --learning_rate 2e-05 --momentum 0.9 --optimizer adamw --regularizers_l2 0.01 --reinitialize_top_layer Auto --rho 0.95 --train_only_top_layer False --validation_split_ratio 0.2 --warmup_steps_fraction 0.1\u001b[0m\n",
      "\u001b[34mExtension horovod.torch has not been built: /usr/local/lib/python3.9/site-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-39-x86_64-linux-gnu.so not found\u001b[0m\n",
      "\u001b[34mIf this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.\u001b[0m\n",
      "\u001b[34mWarning! MPI libs are missing, but python applications are still available.\u001b[0m\n",
      "\u001b[34m2023-08-31 17:33:29.263845: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2023-08-31 17:33:29.264052: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2023-08-31 17:33:29.307645: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34mRunning training scripts with arguments: Namespace(model_dir='/opt/ml/model', train='/opt/ml/input/data/training', validation=None, pretrained_model='/opt/ml/input/data/model', hosts=['algo-1'], current_host='algo-1', verbose_one_line_per_epoch=2, model_url=None, text_processor_url=None, checkpoint_save_best_only='True', seed=123, warmup_steps_fraction=0.1, reinitialize_top_layer='Auto', train_only_top_layer='False', validation_split_ratio=0.2, early_stopping='False', early_stopping_patience=5, early_stopping_min_delta=0.0, dropout_rate=0.2, regularizers_l2=0.01, epochs=4, batch_size=64, optimizer='adamw', learning_rate=2e-05, beta_1=0.9, beta_2=0.999, epsilon=1e-06, momentum=0.9, rho=0.95, initial_accumulator_value=0.1).\u001b[0m\n",
      "\u001b[34mIgnoring unrecognized arguments: [].\u001b[0m\n",
      "\u001b[34mNumber of class labels: 22\u001b[0m\n",
      "\u001b[34mCardinality of dataset 25389\u001b[0m\n",
      "\u001b[34mNumber of training examples: 20311\u001b[0m\n",
      "\u001b[34mNumber of validation examples: 5078\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m'_input_model_extracted/__models_info__.json' file could not be found.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\u001b[0m\n",
      "\u001b[34mNo training configuration found in save file, so the model was *not* compiled. Compile it manually.\u001b[0m\n",
      "\u001b[34mAttaching a randomly initialized classification layer on top of the original encoder layer model to classify input text to one of the 22 classes.\u001b[0m\n",
      "\u001b[34mModel: \"model\"\u001b[0m\n",
      "\u001b[34m__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \u001b[0m\n",
      "\u001b[34m==================================================================================================\u001b[0m\n",
      "\u001b[34mtext (InputLayer)              [(None,)]            0           []\u001b[0m\n",
      "\u001b[34mkeras_layer (KerasLayer)       {'input_type_ids':   0           ['text[0][0]']                   \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_word_ids':\u001b[0m\n",
      "\u001b[34m(None, 128),                                                      \n",
      "                                 'input_mask': (Non                                               \n",
      "                                e, 128)}\u001b[0m\n",
      "\u001b[34mkeras_layer_1 (KerasLayer)     {'sequence_output':  109482241   ['keras_layer[1][0]',            \n",
      "                                 (None, 128, 768),                'keras_layer[1][1]',\u001b[0m\n",
      "\u001b[34m'default': (None,                'keras_layer[1][2]']            \n",
      "                                768),                                                             \n",
      "                                 'encoder_outputs':\u001b[0m\n",
      "\u001b[34m[(None, 128, 768),                                               \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768)],                                               \n",
      "                                 'pooled_output': (                                               \n",
      "                                None, 768)}                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 768)          0           ['keras_layer_1[1][13]']\u001b[0m\n",
      "\u001b[34mclassifier (Dense)             (None, 22)           16918       ['dropout[0][0]']                \n",
      "                                                                                                  \u001b[0m\n",
      "\u001b[34m==================================================================================================\u001b[0m\n",
      "\u001b[34mTotal params: 109,499,159\u001b[0m\n",
      "\u001b[34mTrainable params: 109,499,158\u001b[0m\n",
      "\u001b[34mNon-trainable params: 1\u001b[0m\n",
      "\u001b[34m__________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34musing Adamw optimizer\u001b[0m\n",
      "\u001b[34mgradient_clip_norm=1.000000\u001b[0m\n",
      "\u001b[34mSetting the evaluation metric to: val_accuracy.\u001b[0m\n",
      "\u001b[34mEpoch 1/4\u001b[0m\n",
      "\u001b[34mExtension horovod.torch has not been built: /usr/local/lib/python3.9/site-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-39-x86_64-linux-gnu.so not found\u001b[0m\n",
      "\u001b[34mIf this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.\u001b[0m\n",
      "\u001b[34mWarning! MPI libs are missing, but python applications are still available.\u001b[0m\n",
      "\u001b[34m[2023-08-31 17:33:59.333 ip-10-2-228-194.ec2.internal:37 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.9/site-packages/smdebug-1.0.26b20230119-py3.9.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.9/site-packages/smdebug-1.0.26b20230119-py3.9.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[2023-08-31 17:33:59.640 ip-10-2-228-194.ec2.internal:37 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-08-31 17:33:59.671 ip-10-2-228-194.ec2.internal:37 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-08-31 17:33:59.672 ip-10-2-228-194.ec2.internal:37 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-08-31 17:33:59.672 ip-10-2-228-194.ec2.internal:37 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-08-31 17:33:59.673 ip-10-2-228-194.ec2.internal:37 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-08-31 17:33:59.673 ip-10-2-228-194.ec2.internal:37 INFO hook.py:427] Monitoring the collections: sm_metrics, losses, metrics\u001b[0m\n",
      "\u001b[34m318/318 - 283s - loss: 2.3161 - accuracy: 0.4784 - val_loss: 1.5643 - val_accuracy: 0.6743 - 283s/epoch - 889ms/step\u001b[0m\n",
      "\u001b[34mEpoch 2/4\u001b[0m\n",
      "\u001b[34m318/318 - 255s - loss: 1.4735 - accuracy: 0.6977 - val_loss: 1.3932 - val_accuracy: 0.7085 - 255s/epoch - 803ms/step\u001b[0m\n",
      "\u001b[34mEpoch 3/4\u001b[0m\n",
      "\u001b[34m318/318 - 255s - loss: 1.3041 - accuracy: 0.7390 - val_loss: 1.3587 - val_accuracy: 0.7168 - 255s/epoch - 803ms/step\u001b[0m\n",
      "\u001b[34mEpoch 4/4\u001b[0m\n",
      "\u001b[34m318/318 - 255s - loss: 1.2141 - accuracy: 0.7632 - val_loss: 1.3383 - val_accuracy: 0.7251 - 255s/epoch - 803ms/step\u001b[0m\n",
      "\u001b[34mSetting weights to model achieving the maximum val_accuracy: 0.7250885963439941 at epoch 4/4\u001b[0m\n",
      "\u001b[34mSaving the model with the highest val_accuracy for running inference or incremental training.\u001b[0m\n",
      "\u001b[34mFound untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 364). These functions will not be directly callable after loading.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Assets written to: /opt/ml/model/1/assets\u001b[0m\n",
      "\u001b[34mAssets written to: /opt/ml/model/1/assets\u001b[0m\n",
      "\u001b[34mInfo file not found at '_input_model_extracted/__models_info__.json'.\u001b[0m\n",
      "\u001b[34m2023-08-31 17:51:48,975 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-08-31 17:51:48,975 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-08-31 17:51:48,977 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-08-31 17:51:58 Uploading - Uploading generated training model\n",
      "2023-08-31 17:52:39 Completed - Training job completed\n",
      "Training seconds: 1442\n",
      "Billable seconds: 1442\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "\n",
    "training_job_name = name_from_base(f\"jumpstart-example-{model_id}-transfer-learning\")\n",
    "\n",
    "# Create SageMaker Estimator instance\n",
    "tc_estimator = Estimator(\n",
    "    role=aws_role,\n",
    "    image_uri=train_image_uri,\n",
    "    source_dir=train_source_uri,\n",
    "    model_uri=train_model_uri,\n",
    "    entry_point=\"transfer_learning.py\",\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    max_run=360000,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=s3_output_location,\n",
    "    base_job_name=training_job_name,\n",
    ")\n",
    "\n",
    "if use_amt:\n",
    "    metric_definitions = next(\n",
    "        value for key, value in metric_definitions_per_model.items() if model_id.startswith(key)\n",
    "    )\n",
    "\n",
    "    hp_tuner = HyperparameterTuner(\n",
    "        tc_estimator,\n",
    "        metric_definitions[\"metrics\"][0][\"Name\"],\n",
    "        hyperparameter_ranges,\n",
    "        metric_definitions[\"metrics\"],\n",
    "        max_jobs=max_jobs,\n",
    "        max_parallel_jobs=max_parallel_jobs,\n",
    "        objective_type=metric_definitions[\"type\"],\n",
    "        base_tuning_job_name=training_job_name,\n",
    "    )\n",
    "\n",
    "    # Launch a SageMaker Tuning job to search for the best hyperparameters\n",
    "    hp_tuner.fit({\"training\": training_dataset_s3_path})\n",
    "else:\n",
    "    # Launch a SageMaker Training job by passing s3 path of the training data\n",
    "    tc_estimator.fit({\"training\": training_dataset_s3_path}, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74121803",
   "metadata": {},
   "source": [
    "## 4.5. Deploy & run Inference on the fine-tuned model\n",
    "***\n",
    "A trained model does nothing on its own. We now want to use the model to perform inference. For this example, that means predicting the class label of an input sentence. We follow the same steps as in [3. Run inference on the pre-trained model](#3.-Run-inference-on-the-pre-trained-model). We start by retrieving the jumpstart artifacts for deploying an endpoint. However, instead of base_predictor, we  deploy the `tc_estimator` that we fine-tuned.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6810ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary Python version: py39.\n",
      "INFO:sagemaker:Repacking model artifact (s3://sagemaker-us-east-1-305283204878/jumpstart-tc-training/output/jumpstart-example-tensorflow-tc-bert-en-2023-08-31-17-26-54-055/output/model.tar.gz), script artifact (s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/tensorflow/inference/tc/v2.0.0/sourcedir.tar.gz), and dependencies ([]) into single tar.gz file located at s3://sagemaker-us-east-1-305283204878/sagemaker-jumpstart-2023-08-31-17-54-37-003/model.tar.gz. This may take some time depending on model size...\n",
      "INFO:sagemaker:Creating model with name: sagemaker-jumpstart-2023-08-31-17-54-37-003\n",
      "INFO:sagemaker:Creating endpoint-config with name jumpstart-example-FT-tensorflow-tc-bert-2023-08-31-17-54-37-003\n",
      "INFO:sagemaker:Creating endpoint with name jumpstart-example-FT-tensorflow-tc-bert-2023-08-31-17-54-37-003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----!"
     ]
    }
   ],
   "source": [
    "inference_instance_type = \"ml.m5.xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "# Retrieve the inference script uri\n",
    "deploy_source_uri = script_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, script_scope=\"inference\"\n",
    ")\n",
    "\n",
    "endpoint_name = name_from_base(f\"jumpstart-example-FT-{model_id}-\")\n",
    "\n",
    "# Use the estimator from the previous step to deploy to a SageMaker endpoint\n",
    "finetuned_predictor = (hp_tuner if use_amt else tc_estimator).deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    entry_point=\"inference.py\",\n",
    "    image_uri=deploy_image_uri,\n",
    "    source_dir=deploy_source_uri,\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7f7a0a",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we input example sentences for running inference.\n",
    "These examples are taken from SST2 dataset downloaded from [TensorFlow](https://www.tensorflow.org/datasets/catalog/glue#gluesst2). [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). [Dataset Homepage](https://nlp.stanford.edu/sentiment/index.html). \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ce1baed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text1 = \"astonishing ... ( frames ) profound ethical and philosophical questions in the form of dazzling pop entertainment\"\n",
    "# text2 = \"simply stupid , irrelevant and deeply , truly , bottomlessly cynical \"\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "predictor_input = df_test.drop('cmdb_ci', axis=1).short_description.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8abe5fba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"trying to call southbay clinic (ccsb clinic) and it's asking for ID and pin\",\n",
       "       '++logged into Stanford VPN to access lab\\'s server, gene. However, whenever when trying to access the server through the powershell, we receive an time out error. \"ssh: connect to host gene.stanford.edu port 22: Connection timed out\"',\n",
       "       '* [ONSITE - ORA - CARD HALL LOBBY] Latitude 7420 not working - duplicate ticket of INC01804449 (ORA-LOANER-01 assigned) (loaner handoff appt. 1/31 @ 11:30 AM)',\n",
       "       ..., 'Updating SUNet password and Cardinal Key questions',\n",
       "       'SUNet ID PW Reset',\n",
       "       'Citrix workspace issue | 800 Welch West | CCTO 3rd floor Rad Dept'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fac9b2",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we query the finetuned model, parse the response and print the predictions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "27f2cc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "728ba891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3700/3700 [12:38<00:00,  4.88it/s]\n"
     ]
    }
   ],
   "source": [
    "newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
    "\n",
    "\n",
    "def query_endpoint(encoded_text):\n",
    "    response = finetuned_predictor.predict(\n",
    "        encoded_text, {\"ContentType\": \"application/x-text\", \"Accept\": \"application/json;verbose\"}\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_response(query_response):\n",
    "    model_predictions = json.loads(query_response)\n",
    "    probabilities, labels, predicted_label = (\n",
    "        model_predictions[\"probabilities\"],\n",
    "        model_predictions[\"labels\"],\n",
    "        model_predictions[\"predicted_label\"],\n",
    "    )\n",
    "    return probabilities, labels, predicted_label\n",
    "\n",
    "predictions = []\n",
    "confidence = []\n",
    "\n",
    "for i in tqdm(range(len(predictor_input))):\n",
    "    text = predictor_input[i]\n",
    "    query_response = query_endpoint(text.encode(\"utf-8\"))\n",
    "    probabilities, labels, predicted_label = parse_response(query_response)\n",
    "    predictions.append(predicted_label)\n",
    "    confidence.append(probabilities[int(predicted_label)])\n",
    "#     print(\n",
    "#         f\"Inference:{newline}\"\n",
    "#         f\"Input text: '{text}'{newline}\"\n",
    "#         f\"Model prediction: {probabilities}{newline}\"\n",
    "#         f\"Labels: {labels}{newline}\"\n",
    "#         f\"Predicted Label: {bold}{predicted_label}{unbold}{newline}\"\n",
    "#         f\"Confidence: \n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cdbc641a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_coverage_accuracy(true_labels, predicted_labels_and_scores, eval_fn):\n",
    "    thresholds = [0.70, 0.75, 0.80, 0.85, 0.90, 0.95]\n",
    "    print(\"Threshold     |     Coverage     |     Accuracy    \")\n",
    "    for threshold in thresholds:\n",
    "        indexes = [i for i in range(len(predicted_labels_and_scores)) if float(predicted_labels_and_scores[i][1]) >= threshold]\n",
    "        \n",
    "        coverage = len(indexes) / 3700\n",
    "        \n",
    "        true_covered = []\n",
    "        pred_covered = []\n",
    "        \n",
    "        for index in indexes:\n",
    "            true_covered.append(true_labels[index])\n",
    "            pred_covered.append(predicted_labels_and_scores[index][0])\n",
    "        \n",
    "        score = eval_fn(true_covered, pred_covered)\n",
    "        \n",
    "        print(threshold, \"    |     \", coverage, \"    |    \", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a3198865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_coverage_ci(true_labels, predicted_labels, scores, le):\n",
    "    unique_labels = np.unique(true_labels)\n",
    "    coverage_ci = {}\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        y_true_bool = true_labels == label\n",
    "        y_pred_bool = predicted_labels == label\n",
    "        covered_scores = scores >= 0.75\n",
    "        correct_preds = y_true_bool & y_pred_bool\n",
    "        \n",
    "        covered_and_correct = covered_scores & correct_preds\n",
    "        \n",
    "        \n",
    "        label_accuracy = np.sum(covered_and_correct) / np.sum(y_true_bool & covered_scores)\n",
    "        coverage_ci[le.inverse_transform([label])[0]] = label_accuracy\n",
    "    \n",
    "    return coverage_ci    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4b5cfcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold     |     Coverage     |     Accuracy    \n",
      "0.7     |      0.657027027027027     |     0.8819415878239407\n",
      "0.75     |      0.6167567567567568     |     0.8965819456617002\n",
      "0.8     |      0.5659459459459459     |     0.9140401146131805\n",
      "0.85     |      0.5202702702702703     |     0.9324675324675324\n",
      "0.9     |      0.46054054054054056     |     0.9471830985915493\n",
      "0.95     |      0.3608108108108108     |     0.9737827715355806\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "test_label_encoder = LabelEncoder()\n",
    "y_true = test_label_encoder.fit_transform(df_test.cmdb_ci)\n",
    "y_pred = predictions\n",
    "confidence = np.array(confidence)\n",
    "\n",
    "predicted_label_and_scores = list(zip(y_pred, confidence))\n",
    "measure_coverage_accuracy(y_true, predicted_label_and_scores, accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "af565add",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_accuracies = measure_coverage_ci(y_true, y_pred, confidence, test_label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "87963b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0.98, 0.8085106382978723, 0.9974025974025974, 0.9547511312217195, 0.06818181818181818, 0.9507042253521126, 0.0, 0.09090909090909091, 0.5833333333333334, 0.9469026548672567, 0.9117647058823529, 0.7368421052631579, 0.8571428571428571, 0.8648648648648649, 0.9714285714285714, 0.9722222222222222, 0.88, 0.8, 1.0, 0.9761904761904762, 0.9831932773109243, 1.0])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_accuracies.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89667923",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we clean up the deployed endpoint.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "32cdae4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: sagemaker-jumpstart-2023-08-31-17-54-37-003\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: jumpstart-example-FT-tensorflow-tc-bert-2023-08-31-17-54-37-003\n",
      "INFO:sagemaker:Deleting endpoint with name: jumpstart-example-FT-tensorflow-tc-bert-2023-08-31-17-54-37-003\n"
     ]
    }
   ],
   "source": [
    "# Delete the SageMaker endpoint and the attached resources\n",
    "finetuned_predictor.delete_model()\n",
    "finetuned_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1445fe2",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-1/introduction_to_amazon_algorithms|jumpstart_text_classification|Amazon_JumpStart_Text_Classification.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-2/introduction_to_amazon_algorithms|jumpstart_text_classification|Amazon_JumpStart_Text_Classification.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-1/introduction_to_amazon_algorithms|jumpstart_text_classification|Amazon_JumpStart_Text_Classification.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ca-central-1/introduction_to_amazon_algorithms|jumpstart_text_classification|Amazon_JumpStart_Text_Classification.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/sa-east-1/introduction_to_amazon_algorithms|jumpstart_text_classification|Amazon_JumpStart_Text_Classification.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-1/introduction_to_amazon_algorithms|jumpstart_text_classification|Amazon_JumpStart_Text_Classification.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-2/introduction_to_amazon_algorithms|jumpstart_text_classification|Amazon_JumpStart_Text_Classification.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-3/introduction_to_amazon_algorithms|jumpstart_text_classification|Amazon_JumpStart_Text_Classification.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-central-1/introduction_to_amazon_algorithms|jumpstart_text_classification|Amazon_JumpStart_Text_Classification.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-north-1/introduction_to_amazon_algorithms|jumpstart_text_classification|Amazon_JumpStart_Text_Classification.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-1/introduction_to_amazon_algorithms|jumpstart_text_classification|Amazon_JumpStart_Text_Classification.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-2/introduction_to_amazon_algorithms|jumpstart_text_classification|Amazon_JumpStart_Text_Classification.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-1/introduction_to_amazon_algorithms|jumpstart_text_classification|Amazon_JumpStart_Text_Classification.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-2/introduction_to_amazon_algorithms|jumpstart_text_classification|Amazon_JumpStart_Text_Classification.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-south-1/introduction_to_amazon_algorithms|jumpstart_text_classification|Amazon_JumpStart_Text_Classification.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
